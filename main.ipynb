{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Prepare — What do We Need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Description of Data\n",
    "\n",
    ">Available on hands<br>\n",
    "Content : Details of every ride logged by Cyclistic customers<br>\n",
    "Range of Data : 2013 - 2024 Mar<br>\n",
    "\n",
    "> Used in project<br>\n",
    "Content : Details of every ride logged by Cyclistic customers<br>\n",
    "Range of Data : 2023 Apr - 2024 Mar `Past 12 months`<br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Credibility of Data\n",
    "\n",
    "The credibility and integrity of our data can be determined using the ROCCC system.\n",
    "\n",
    "Reliable — it has a large sample size, reflecting the population size.\n",
    "Original — we can locate the primary source.\n",
    "Comprehensive — it is understandable and does not contain any missing critical information needed to answer the question or find the solution, nor does it have human error.\n",
    "Current — it is relevant and up to date, thus indicating that the source refreshes its data regularly.\n",
    "Cited — the source has been vetted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Limitations of Data\n",
    "\n",
    "Data privacy issues prohibit using riders' personally identifiable information such as gender and age, it means that we cannot provide relationship between cutsomers' characteristic such as geographic and demoographic information to customers' behavioural.\n",
    "\n",
    "Besides, there are no data on hand about pricing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Process — From Dirty to Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision of tool\n",
    "\n",
    "Tool: Python and Tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 10)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# # To plot figures\n",
    "# %matplotlib inline\n",
    "# import matplotlib as mpl\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import seaborn as sns\n",
    "\n",
    "# Common imports\n",
    "import os\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locate the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory where the CSV files are stored\n",
    "directory = r'\\input'\n",
    "\n",
    "# Optionally, define a pattern if the files are consistently named\n",
    "file_pattern = '-divvy-tripdata.csv'\n",
    "\n",
    "# Read a file as sample\n",
    "df_sample = pd.read_csv(r'input\\202304-divvy-tripdata.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the basic info. of the sample\n",
    "df_sample.info()\n",
    "\n",
    "# In view of the basic info., there are large number of 'null' items in some columns, \n",
    "# We can filter some significant column to increase the understanding of the database \n",
    "df_sample[df_sample['start_station_name'].notna() & df_sample['end_station_name'].notna()].head(5) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess the impact of the missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of rows\n",
    "def impact_of_missing_value(df):\n",
    "    total_rows = df.shape[0]\n",
    "\n",
    "    # Calculate the number of missing values per column\n",
    "    missing_counts = df.isnull().sum()\n",
    "\n",
    "    # Calculate the percentage of missing values per column\n",
    "    missing_percentage = round((missing_counts / total_rows) * 100,2)\n",
    "\n",
    "    # Calculate the percentage of non-missing values per column\n",
    "    non_missing_percentage = 100 - missing_percentage\n",
    "\n",
    "    # Create a DataFrame to nicely display the results\n",
    "    data_loss_df = pd.DataFrame({\n",
    "        'Total Rows': total_rows,\n",
    "        'Missing Values': missing_counts,\n",
    "        'Percentage Missing': missing_percentage,\n",
    "        'Percentage Non-Missing': non_missing_percentage\n",
    "    })\n",
    "\n",
    "    \n",
    "    return data_loss_df\n",
    "print(impact_of_missing_value(df_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_status_checking(df):\n",
    "    # Number of cases where 'end_station_name' is missing but 'start_station_name' is not missing\n",
    "    end_missing_start_not = df[df['end_station_name'].isnull() & df['start_station_name'].notnull()].shape[0]\n",
    "\n",
    "    # Number of cases where 'start_station_name' is missing but 'end_station_name' is not missing\n",
    "    start_missing_end_not = df[df['start_station_name'].isnull() & df['end_station_name'].notnull()].shape[0]\n",
    "\n",
    "    # Number of cases where both 'start_station_name' and 'end_station_name' are missing\n",
    "    both_missing = df[df['start_station_name'].isnull() & df['end_station_name'].isnull()].shape[0]\n",
    "    \n",
    "\n",
    "    print(f\"End station name missing, start station name not missing: {end_missing_start_not}\")\n",
    "    print(f\"Start station name missing, end station name not missing: {start_missing_end_not}\")\n",
    "    print(f\"Both start and end station names missing: {both_missing}\")\n",
    "\n",
    "missing_status_checking(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplicate(df):\n",
    "    duplicate_status = df.duplicated().any()\n",
    "    print(f'Duplicate found : {duplicate_status}')\n",
    "    return\n",
    "\n",
    "check_duplicate(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling null data\n",
    "\n",
    "Regards `null` value in `end_lat`, we can check on the start point and find any reason for missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_by_station = df_sample[df_sample['end_lat'].isnull()].groupby('start_station_name').size()\n",
    "\n",
    "# Sort the counts in descending order\n",
    "sorted_count_by_station = count_by_station.sort_values(ascending=False)\n",
    "\n",
    "print(sorted_count_by_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all NaN values with 'unknown'\n",
    "def df_fillna_unknown (df):\n",
    "    df_sample_filled = df.fillna('unknown')\n",
    "    print(f'Null values items:\\n{df_sample_filled.isnull().sum()}')\n",
    "    return df_sample_filled\n",
    "\n",
    "df_sample_filled = df_fillna_unknown(df_sample)\n",
    "print(df_fillna_unknown(df_sample).head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Data Transformation\n",
    "\n",
    "#### Column on hand\n",
    "\n",
    "1. `ride_id`: Unique ID assigned with each ride\n",
    "2. `rideable_type`: Type of bicycle used on each ride — classic, docked, or electric\n",
    "3. `started_at`: Date and time at the start of each trip\n",
    "4. `ended_at`: Date and time at the end of each trip\n",
    "5. `start_station_name`: Name of the station where each journey started from\n",
    "6. `start_station_id`: ID of the station where each journey started from\n",
    "7. `end_station_name`: Name of the station where each trip ended at\n",
    "8. `end_station_id`: ID of the station where each trip ended at\n",
    "9. `start_lat`: Latitude of each starting station\n",
    "10. `start_lng`: Longitude of each starting station\n",
    "11. `end_lat`: Latitude of each ending station\n",
    "12. `end_lng`: Longitude of each ending station\n",
    "13. `member_casual`: Type of membership of each rider\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional:\n",
    "- `ride_length` : Length of each ride\n",
    "- `ride_length_minutes` : Length of each ride in minutes\n",
    "- `start_hour` : Time in hour for starting each ride\n",
    "- `weekday_name` : Weekday of each ride\n",
    "- `ride_length_minutes_category` : Dividing in different minitues category to understand the usage\n",
    "- `ride_length_category` : Dividing in diiferent category to understand the purpose of usage\n",
    "\n",
    "#### Not in used\n",
    " - `trip_distance` : There are chance of criculation ride instead of point to point ride. For example, the starting point is so close to the end point such as riding in a park.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def processing_data(df):\n",
    "    # Convert 'started_at' and 'ended_at' to datetime\n",
    "    df['started_at'] = pd.to_datetime(df['started_at'])\n",
    "    df['ended_at'] = pd.to_datetime(df['ended_at'])\n",
    "\n",
    "    # Calculate ride length\n",
    "    df['ride_length'] = df['ended_at'] - df['started_at']\n",
    "\n",
    "    # Optional: Convert ride length to minutes\n",
    "    df['ride_length_minutes'] = df['ride_length'].dt.total_seconds() / 60\n",
    "    \n",
    "    df['start_hour'] = df['started_at'].dt.hour\n",
    "    print(df['start_hour'].unique())\n",
    "\n",
    "    df['weekday_name'] = df['started_at'].dt.day_name()\n",
    "    print(df['weekday_name'].unique())\n",
    "\n",
    "    # Filter out rides with duration less than 1 minute or more than 720 minutes\n",
    "    df_filtered = df[(df['ride_length_minutes'] >= 1) & (df['ride_length_minutes'] <= 720)]\n",
    "\n",
    "    # Report dropped entries\n",
    "    dropped_entries = len(df) - len(df_filtered)\n",
    "    print(f'Entries dropped for being outside 1-720 minutes: {dropped_entries}')\n",
    "    print(f'Percentage dropped: {100 * dropped_entries / len(df):.2f}%')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Define bins for the ride length categories\n",
    "    bins_ride_mins = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 90, 120, float('inf')]\n",
    "    labels_ride_mins = [\n",
    "        '1-5 mins', '5-10 mins', '10-15 mins', '15-20 mins', \n",
    "        '20-25 mins', '25-30 mins', '30-35 mins', '35-40 mins', \n",
    "        '40-45 mins', '45-50 mins', '50-55 mins', '55-60 mins', \n",
    "        '60-90 mins', '90-120 mins', '120+ mins'\n",
    "    ]\n",
    "\n",
    "    # Define the bins and labels\n",
    "    bins_ride_cate = [0, 5, 15, 30, 60, float('inf')]\n",
    "    labels_ride_cate = [\"Very Short (1-5 mins)\", \"Short (6-15 mins)\", \"Moderate (16-30 mins)\", \"Long (31-60 mins)\", \"Very Long (60+ mins)\"]\n",
    "\n",
    "    # Create a new column 'ride_length_category'\n",
    "    df_filtered['ride_length_minutes_category'] = pd.cut(df_filtered['ride_length_minutes'], bins=bins_ride_mins, labels=labels_ride_mins, right=False)\n",
    "    df_filtered['ride_length_category'] = pd.cut(df_filtered['ride_length_minutes'], bins=bins_ride_cate, labels=labels_ride_cate, right=False)\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "df_sample_filled = processing_data(df_sample_filled)\n",
    "print(df_sample_filled.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the sample, start to merge the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_sample\n",
    "del df_sample_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Merge Data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In case you place the whole dataset in difference place\n",
    "directory = r'\\input'\n",
    "\n",
    "# The same with the sample\n",
    "file_pattern = '-divvy-tripdata.csv'\n",
    "\n",
    "dataframes = []  # List to store each DataFrame\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('-divvy-tripdata.csv'):\n",
    "        print(\"Processing file:\", filename)\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes.append(df)\n",
    "    else:\n",
    "        print(\"Skipping file:\", filename)\n",
    "\n",
    "# Combine all DataFrames into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "print(\"Combined DataFrame shape:\", combined_df.shape)\n",
    "\n",
    "# If you want to save the conbined_df as csv\n",
    "# combined_df.to_csv('combined_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reperformance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i. Check the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii. Check the impact of missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(impact_of_missing_value(combined_df))\n",
    "missing_status_checking(combined_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### iii. Check duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_duplicate(combined_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### iv. Fill the null value with 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_fillna_unknown(combined_df).head())\n",
    "df_merged_filled = df_fillna_unknown(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### v. Processing the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_filled = processing_data(df_merged_filled)\n",
    "df_merged_filled.info()\n",
    "print(df_merged_filled.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### vi. Describe the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_filled.describe(include='all')\n",
    "print(df_merged_filled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### vii. Output the dataframe to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_filled.to_csv('processed_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
